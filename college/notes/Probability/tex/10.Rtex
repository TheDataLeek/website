\documentclass[11pt]{article}

\input{./tex/header.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beginning of document items - headers, title, toc, etc...
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}                                                       %  Establishes that the headers will be defined
\fancyhead[LE,LO]{Homework 10}                                  %  Adds header to left
\fancyhead[RE,RO]{Zoe Farmer}                                       %  Adds header to right
\cfoot{\mlptikz[size=0.25in, text=on, textposx=0, textposy=0, textvalue=\thepage, textscale=0.75in]{applejack}}
\lfoot{APPM 3570}
\rfoot{Kleiber}
\title{Homework 10}
\date{Kleiber}
\author{Zoe Farmer - 101446930}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beginning of document items - headers, title, toc, etc...
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
<<globals, echo=F, message=F, tidy=F>>=
    library('ggplot2')
    library('plot3D')
@

\maketitle

\begin{table}[!ht]
    \centering
    \scalebox{1.5}{%
    \begin{tabular}{|l|l|l|l||l|}
        \hline
        1 & 2 & 3 & 4 & T\\
        \hline
        & & & &\\
        \hline
    \end{tabular}
    }
\end{table}

\begin{easylist}[enumerate]
    \ListProperties(Hide1=50, Space1=1cm)
    @ \textit{Chapter 6, \#2 --} Suppose that 3 balls are chosen without replacement from an urn consisting of 5 white
    and 8 red balls. Let $X_i$ equal 1 if the $i$th ball selected is white, and let it equal 0 otherwise. Give the joint
    probability mass function of
    @@ $X_0, X_1$
    @@@ Each instance of $X_i$ can be thought of as a random variable with binomial distribution with $n=1$ and varying
    $p = \frac{5 - i}{13 - i}$. We can now use these values to find the probability of any $X_i$ being a success or
    failure. If $k=1$, then we call this a success (white ball chosen), otherwise if $k=0$ we denote this as a failure
    (black ball chosen). We know the probability of the first ball, and we can establish our equation.

    \[
        P_1(X_1) =
        \begin{cases}
            { \left( \frac{5}{13} \right)}^{X_1} { \left( \frac{8}{13} \right) }^{1 - X_1} &\to X_1 \in \{ 0, 1 \}\\
            0 &\to Otherwise
        \end{cases}
    \]

    We can now set up our second probability similarly based on this result. First we need to establish our probability
    of success, which is dependent on $X_1$.

    \[
        p = { \left( \frac{4}{12} \right) }^{X_1} { \left( \frac{5}{12} \right) }^{1 - X_1}
    \]

    This answer is determined by noting that if $X_1 = 1$, and indicates success, we need to reduce the probability to
    $4/12$, however if $X_1$ is a failure, we adjust to $5/12$. Notice the inherent similarity to the Binomial
    Distribution.\newline

    Now we can find the joint probability.

    \[
        f(X_1, X_2) = { \left( { \left( \frac{4}{12} \right) }^{X_1} { \left( \frac{5}{12} \right) }^{1 - X_1} \right) }^{X_2}
        { \left( 1 - { \left( \frac{4}{12} \right) }^{X_1} { \left( \frac{5}{12} \right) }^{1 - X_1} \right) }^{1 - X_2}
    \]

    <<6.2prob.a, echo=F>>=
        prob.a <- function(x1, x2) {
            ((4/12)^x1 * (5/12)^(1-x1))^x2 * ( 1 - (4/12)^x1 * (5/12)^(1-x1))^(1-x2)
        }
    @

    Looking at our table of probabilities, we can see the exact probabilities. Please reference
    Table~\ref{table:probs.a}

    \begin{table}
        \centering
        \begin{tabular}{|l|l|l|}
            \hline
            $X_2$ & \multicolumn{2}{|c|}{$X_1$}\\
            \hline
            & 0 & 1\\
            \hline
            0 & $\Sexpr{prob.a(0, 0)}$ & $\Sexpr{prob.a(0, 1)}$\\
            1 & $\Sexpr{prob.a(1, 0)}$ & $\Sexpr{prob.a(1, 1)}$\\
            \hline
        \end{tabular}
        \caption{Table of Probabilities for $f(X_1, X_2)$}
        \label{table:probs.a}
    \end{table}

    @@ $X_1, X_2, X_3$
    @@@ We can build off of the previous problem to solve this one. We've already determine the joint distribution for
    $X_1, X_2$, so now we can again start by determining the probability of success for $X_3$.

    \[
        p = { \left( \frac{3}{11} \right) }^{X_1 X_2}
            { \left( \frac{4}{11} \right) }^{\abs{X_1 - X_2}}
            { \left( \frac{5}{11} \right) }^{1 - X_1 - X_2 + X_1 X_2}
    \]

    Plugging into our equation we get the following.

    \[
        \begin{aligned}
            f(X_1, X_2, X_3) =& { \left( { \left( \frac{3}{11} \right) }^{X_1 X_2}
                                        { \left( \frac{4}{11} \right) }^{\abs{X_1 - X_2}}
                                        { \left( \frac{5}{11} \right) }^{1 - X_1 - X_2 + X_1 X_2}
                                 \right) }^{X_3} \cdot \\
                              & { \left( 1 - { \left( \frac{3}{11} \right) }^{X_1 X_2}
                                        { \left( \frac{4}{11} \right) }^{\abs{X_1 - X_2}}
                                        { \left( \frac{5}{11} \right) }^{1 - X_1 - X_2 + X_1 X_2}
                                 \right) }^{1 - X_3}
         \end{aligned}
    \]

    <<6.2prob.b, echo=F>>=
        prob.b <- function(x1, x2, x3) {
            p <- ((3/11)^(x1*x2)) *
                 ((4/11)^(abs(x1 - x2))) *
                 ((5/11)^(1 - x1 - x2 + x1*x2))
            p^x3 * (1 - p)^(1 - x3)
        }
    @

    We can make another table. (Table~\ref{table:probs.b})

    \begin{table}
        \centering
        \begin{tabular}{|l|l|l|l|l|}
            \hline
            $X_2$ & \multicolumn{4}{|c|}{$X_1$}\\
            \hline
            & \multicolumn{2}{|c|}{0} & \multicolumn{2}{|c|}{1}\\
            \hline
            & $X_3 = 0$ & $X_3 = 1$ & $X_3 = 0$ & $X_3 = 1$\\
            \hline
            0 & $\Sexpr{prob.b(0, 0, 0)}$ & $\Sexpr{prob.b(0, 0, 1)}$ & $\Sexpr{prob.b(1, 0, 0)}$ & $\Sexpr{prob.b(1, 0, 1)}$\\
            1 & $\Sexpr{prob.b(0, 1, 0)}$ & $\Sexpr{prob.b(0, 1, 1)}$ & $\Sexpr{prob.b(1, 1, 0)}$ & $\Sexpr{prob.b(1, 1, 1)}$\\
            \hline
        \end{tabular}
        \caption{Table of Probabilities for $f(X_1, X_2, X_3)$}
        \label{table:probs.b}
    \end{table}

    @ \textit{Chapter 6, \#4 --} Repeat Problem 2 when the ball selected is replaced in the urn before the next
    selection.
    @@ If every ball is replaced, suddenly our problem becomes much easier, as each draw is now independent of any other
    draw, and we can simply examine each probability independently. The probability for any draw is

    \[
        P_i(X_i) = \frac{5}{13}
    \]

    So in order to determine the joint probability for $X_1$ and $X_2$ we find the probability of each draw.

    \[
        f(X_1, X_2) = { \left( \frac{5}{13} \right) }^{X_1 + X_2} { \left( 1 - \frac{5}{13} \right) }^{2 - X_1 - X_2}
    \]

    Now we can extend this and allow for all three.

    \[
        f(X_1, X_2, X_3) = { \left( \frac{5}{13} \right) }^{X_1 + X_2 + X_3}
                           { \left( 1 - \frac{5}{13} \right) }^{3 - X_1 - X_2 - X_3}
    \]

    @ \textit{Chapter 6, \#7 --} Consider a sequence of independent Bernoulli trials, each of which is a success with
    probability $p$. Let $X_1$ be the number of failures preceding the first success, and let $X_2$ be the number of
    failures between the first two successes. Find the joint mass function of $X_1$ and $X_2$.
    @@ We know that $X_1$ is the number of failures before the first success, and $X_2$ is the number of failures between
    the first and the second, therefore we can construct our two random variables.

    \[
        f(X_1; p) =
        \begin{cases}
            { \left( 1 - p \right)}^{X_1} & \to {X_1} \in { \left\{ 0, 1, 2, \ldots \right\} }\\
            0 &\to Otherwise
        \end{cases}
    \]

    The second random variable is constructed similarly to the first.

    \[
        f(X_2; p) =
        \begin{cases}
            { \left( 1 - p \right)}^{X_2} & \to {X_2} \in { \left\{ 0, 1, 2, \ldots \right\} }\\
            0 &\to Otherwise
        \end{cases}
    \]

    Since the trials are independent, these two random variables are independent, meaning to find the joint probability
    we can simply multiply them together.

    \[
        f(X_1, X_2; p) =
        \begin{cases}
            { \left( 1 - p \right)}^{X_1 + X_2} & \to X_1, X_2 \in { \left\{ 0, 1, 2, \ldots \right\} }\\
            0 &\to Otherwise
        \end{cases}
    \]

    @ \textit{Chapter 6, \#8 --} The joint probability density function of $X$ and $Y$ is given by

    \[
        f(x, y) = c (y^2 - x^2) e^{-y} \qquad -y \le x \le y, 0 < y < \infty
    \]

    @@ Find $c$.
    @@@ We know that the cumulative distribution function must equal 1 when integrated from $-\infty$ to $\infty$.

    \[
        \int \int_{-y}^y c (y^2 - x^2) e^{-y} \, dx \, dy =
        \left. \frac{4}{3} c e^{-y} \left(-y^3-3 y^2-6 y-6\right) \right|^\infty_0 =
        8c
    \]

    Therefore $\boxed{c = 1/8}$.

    @@ Find the marginal densities of $X$ and $Y$.
    @@@ This is simply integration.

    \[
        \begin{aligned}
            f_X(x) &=& \int_0^\infty
            \frac{1}{8} (y^2 - x^2) e^{-y}
            \, dy &=& \frac{1}{8} \left(2 - x^2\right)\\
            f_Y(y) &=& \int_{-y}^y
            \frac{1}{8} (y^2 - x^2) e^{-y}
            \, dx &=& \frac{1}{6} e^{-y} y^3
        \end{aligned}
    \]

    @@ Find $E[X]$.
    @@@ In the previous problem we found our marginal for $X$, and now we can find the expected value.

    \[
        \begin{aligned}
            \int x \left( \frac{1}{8} \left(2 - x^2\right)\right) \, dx =
            \left. \frac{1}{8} \left(x^2-\frac{x^4}{4}\right) \right|^y_{-y} = \boxed{0}
        \end{aligned}
    \]

    @ \textit{Chapter 6, \#10 --} The joint probability density function of $X$ and $Y$ is given by

    \[
        f(x, y) =
        e^{-(x + y)}
        \qquad 0 \le x \le \infty, 0 < y < \infty
    \]

    @@ Find $P(X < Y)$ 
    @@@ We can first find the marginals.

    \[
        \begin{aligned}
            f_X(x) &=& \int_0^\infty e^{-(x + y)} \, dy &=& e^{-x}\\
            F_X(x) &=& && e^{-x}\\
            f_Y(y) &=& \int_0^\infty e^{-(x + y)} \, dx &=& e^{-y}\\
            F_Y(y) &=& && e^{-y}\\
        \end{aligned}
    \]

    Now we can reevaluate the question and come to the conclusion that

    \[
        \begin{aligned}
            P(X < Y) &=& \int P(X < y) f_Y(y) \, dy = \int F_X(y) f_Y(y) \, dy\\
                     &=& \int e^{-y} \cdot e^{-y} \, dy = \int_0^\infty e^{-2y} \, dy = \boxed{0.5}
        \end{aligned}
    \]

    @@ Find $P(X < a)$
    @@@ We've already calculated this result.

    \[
        F_X(a) = e^{-a}
    \]

    @ \textit{Chapter 6, \#13 --} A man and a woman agree to meet at a certain location about 12:30 PM. If the man
    arrives at a time uniformly distributed between 12:15 and 12:45, and if the woman independently arrives at a time
    uniformly distributed between 12:00 and 1 PM, find the probability that the first to arrive waits no longer than 5
    minutes. What is the probability that the man arrives first?
    @@ We can look at these two distributions of arrival times independently of one another. Let $A_i$ indicate the
    arrival time, where $i \in \left\{ m, w \right\}$.

    \[
        \begin{aligned}
            A_m(m) =
            \begin{cases}
                \frac{1}{30} &\to 15 \le m \le 45\\
                0 &\to Otherwise
            \end{cases}\\
            A_w(w) =
            \begin{cases}
                \frac{1}{60} &\to 0 \le w \le 60\\
                0 &\to Otherwise
            \end{cases}\\
            A(m, w) =
            \begin{cases}
                \frac{1}{1800} &\to 15 \le m \le 45, 15 \le w \le 45\\
                \frac{1}{60} &\to 15 \ge m, 0 \le w \le 60\\
                \frac{1}{60} &\to m \ge 45, 0 \le w \le 60\\
                0 &\to Otherwise
            \end{cases}
        \end{aligned}
    \]

    Let $M$ be the man's arrival time, and $W$ the womans. We're interested in $P(-5 \le M - W \le 5) \Rightarrow 1 -
    P(M - W \le -5) + P(M - W \le 5)$ and $P(M \le W)$.

    <<echo=F>>=
        prob.6.13 <- function(x) { (1/2) - (x/60) }
    @

    \[
        \begin{aligned}
            \int\int_{m - w \le z} \frac{1}{1800} \, dw \, dm \Rightarrow
                \int\int_{w \le m - z} \frac{1}{1800} \, dw \, dm\\
            \int^{45}_{15} \int_{0}^{m - z} \frac{1}{1800} \, dw \, dm\\
            f(z) = \frac{1}{2} - \frac{z}{60} \Rightarrow 1 - f(-5) + f(5) =\\
                \boxed{P(-5 \le M - W \le 5) = \Sexpr{1 - prob.6.13(-5) + prob.6.13(5)}}
        \end{aligned}
    \]

    Now we can look at $P(M \le W) \Rightarrow 1 - P(W \le M)$.

    \[
        \begin{aligned}
            \int \int_{w \le m} \frac{1}{1800} \, dw \, dm \Rightarrow \int \int_{w \le m} \frac{1}{1800} \, dw \, dm\\
            \int_{15}^{45} \int_0^m \frac{1}{1800} \, dw \, dm =\\
            \boxed{P(M \le W) = \frac{1}{2}}
        \end{aligned}
    \]

    @ \textit{Chapter 6, \#20 --} The joint density of $X$ and $Y$ is given by

    \[
        f(x, y) =
        \begin{cases}
            xe^{-(x + y)} &\to x > 0, y > 0\\
            0 &\to Otherwise
        \end{cases}
    \]

    Are $X$ and $Y$ independent? If, instead, $f(x,y)$ were given by

    \[
        f(x, y) =
        \begin{cases}
            2 &\to 0 < x < y, 0 < y < 1\\
            0 &\to Otherwise
        \end{cases}
    \]

    Would $X$ and $Y$ be independent?
    @@ First, let's integrate the joint distribution function twice, varying the variable we integrate with respect to.

    \[
        \begin{aligned}
            p_x(x) &=& \int_0^\infty xe^{-(x + y)} \, dy = xe^{-x}\\
            p_y(y) &=& \int_0^\infty xe^{-(x + y)} \, dx = e^{-y}
        \end{aligned}
    \]

    We can see that $p_x(x) \cdot p_y(y) = xe^{-x} e^{-y} = xe^{-(x + y)} = f(x, y)$, therefore they are independent. If
    we examine our second case, and let $f(x,y)$ be redefined, we can perform the same process.

    \[
        \begin{aligned}
            p_x(x) &=& \int_0^1 2 \, dy = 2\\
            p_y(y) &=& \int_0^y 2 \, dx = 2y
        \end{aligned}
    \]

    After which we come to the conclusion that they are \textit{not} independent, as $4y \neq 2$.

    @ \textit{Chapter 6, \#22 --}  The joint density of $X$ and $Y$ is given by

    \[
        f(x, y) =
        \begin{cases}
            x + y &\to 0 < x < 1, 0 < y < 1\\
            0 &\to Otherwise
        \end{cases}
    \]

    @@ Are $X$ and $Y$ independent?
    @@@ To answer this we find the marginal distributions.

    \[
        \begin{aligned}
            f_X(x) &=& \int_0^1 x + y \, dy &=& x + \frac{1}{2}\\
            f_Y(y) &=& \int_0^1 x + y \, dx &=& y + \frac{1}{2}\\
            f_X(x) \cdot f_Y(y) &=& x y+0.5 x+0.5 y+0.25 &\neq& x + y
        \end{aligned}
    \]

    No, they are not independent.

    @@ Find the density function of $X$.
    @@@ Answered above, and we get $x + 1/2$.
    @@ Find $P(X + Y < 1)$.
    @@@

    \[
        \begin{aligned}
            \int\int_{x + y < 1} x + y \, dx \, dy \Rightarrow \int\int_{x < 1 - y} x + y \, dx \, dy\\
            \int_0^1 \int_0^{1 - y} x + y \, dx \, dy = \boxed{\frac{1}{3}}
        \end{aligned}
    \]

    @ A stockroom currently has 30 components of a certain type, of which 8 components were provided by supplier \#1, 10
    components by supplier \#2 and 12 components by supplier \#3. Six of these are to be selected for a particular
    assembly. Let $X$ be the random variable that represents the number of supplier \#1's components that are selected
    and $Y$ the number of supplier \#2's components that are selected.
    @@ What is $P(X = 3, Y = 2)$?
    @@@ To answer this question we need to determine the joint probability function. Let us assume that the choice of
    part has uniform distribution, that is, every part has a $1/30$ chance of being selected. We also know that
    $P(X=1)=8/30, P(Y=1)=10/30$.

    <<echo=F>>=
        stockprob <- function(x, y) { (4/15)^x * (1/3)^y}
    @

    \[
        P(X=x=3, Y=y=2) = {\left( \frac{4}{15} \right)}^x { \left( \frac{1}{3} \right)}^y = \Sexpr{stockprob(3, 2)}
    \]

    @@ Generalize your answer to part (a) to find the joint probability mass function for $X$ and $Y$.  That is, find
    $P(X = a, Y = b)$ for all values of $a$ and $b$.
    @@@ We already determined this.

    \[
        P(X=x, Y=y) = {\left( \frac{4}{15} \right)}^x { \left( \frac{1}{3} \right)}^y
    \]

    @ As we read in problem \#60 on p.\ 102, the color of a person's eyes is determined by a single pair of genes. Let
    $B$ represent the dominant brown-eyed gene and $b$ represent the recessive blue-eyed gene. Then, $BB$ and $Bb$ will
    all result in brown eyes while $bb$ will result in blue eyes. In a particular population, let $p_1$ be the
    probability that a randomly selected person has $BB$, $p_2$ be the probability of $Bb$, and $p_3$ the probability of
    $bb$. If 10 individuals are randomly selected, what is the probability that 5 are $BB$, 3 are $Bb$ and 2 are $bb$?
    @@ Let's initially assume that each $p_i$ is independent from any other $p_i$, and that $B$ is equally likely as $b$
    from any given parent. This implies that we have a joint distribution function stating the probabilities of eye
    color. Let $X$ be a Bernoulli Random Variable where 1 indicates brown, and 0 indicates blue, letting $Y$ be the
    same, just from the other parent.

    \[
        P(X=x, Y=y) = 0.5 \cdot 0.5 = 0.25
    \]

    We also know that $Bb \equiv bB$, therefore

    \[
        \begin{aligned}
            p_1 &=& 0.25\\
            p_2 &=& 0.5\\
            p_3 &=& 0.25\\
        \end{aligned}
    \]

    Now we can look at our random sampling of 10 individuals.

    <<echo=F>>=
        BB <- (0.25)^5
        Bb <- 0.5^3
        bb <- 0.25^3
        p <- (BB * bb * Bb)
    @

    \[
        \begin{aligned}
            5BB &=& 0.25^5 &=& \Sexpr{BB}\\
            3Bb &=& 0.5^3 &=& \Sexpr{Bb}\\
            2bb &=& 0.25^3 &=& \Sexpr{bb}\\
            P &=& 5BB * 3Bb * 2bb &=& \Sexpr{p}
        \end{aligned}
    \]
\end{easylist}

\end{document}
