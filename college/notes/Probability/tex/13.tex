\documentclass[11pt]{article}\usepackage[]{graphicx}\usepackage[]{xcolor}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1} }%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1} }%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1} } }%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1} }%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1} }%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1} } }%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1} }%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1} }%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1} } }%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage} }%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage} }%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\input{./tex/header.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beginning of document items - headers, title, toc, etc...
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}                                                       %  Establishes that the headers will be defined
\fancyhead[LE,LO]{Homework 13}                                  %  Adds header to left
\fancyhead[RE,RO]{Zoe Farmer}                                       %  Adds header to right
\cfoot{\mlptikz[size=0.25in, text=on, textposx=0, textposy=0, textvalue=\thepage, textscale=0.75in]{applejack} }
\lfoot{APPM 3570}
\rfoot{Kleiber}
\title{Homework 13}
\date{Kleiber}
\author{Zoe Farmer - 101446930}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beginning of document items - headers, title, toc, etc...
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\IfFileExists{upquote.sty}{\usepackage{upquote} }{}
\begin{document}



\maketitle

\begin{table}[H]
    \centering
    \scalebox{1.5}{%
    \begin{tabular}{|l|l|l|l||l|}
        \hline
        1 & 2 & 3 & 4 & T\\
        \hline
        & & & &\\
        \hline
    \end{tabular}
    }
\end{table}

\begin{easylist}[enumerate]
    \ListProperties(Hide1=50, Space1=1cm)
    @ \textit{Chapter 6, \#50} Let $Z_1$ and $Z_2$ be independent standard normal random variables. Show that $X$, $Y$
    has a bivariate normal distribution when $X = Z_1$ , $Y = Z_1 + Z_2$.

    @@ The bivariate normal distribution is defined as

    \begin{equation}
        f(x, y) =
            \frac{%
            \exp \left[
                - \frac{1}{2\left( 1 - \rho^2 \right)}
                \left(
                    {\left[ \frac{x - \mu_x}{\sigma_x} \right]}^2 +
                    {\left[ \frac{y - \mu_y}{\sigma_y} \right]}^2 -
                    2 \rho \frac{(x - \mu_x)(y - \mu_y)}{\sigma_x\sigma_y}
                \right)
            \right]}{2\pi \sigma_x \sigma_y \sqrt{1 - \rho^2} }
    \end{equation}

    Whereas the standard normal distribution is defined as

    \begin{equation}
        f(x) = \frac{1}{\sqrt{2\pi} } e^{ -\frac{x^2}{2} }
    \end{equation}

    Let $g_1(z_1, z_2) = z_1$ and $g_2(z_1, z_2) = z_1 + z_2$, therefore the Jacobian is defined as

    \begin{equation}
        \begin{aligned}
            J(z_1, z_2) = \left| \begin{pmatrix}
                                     1 & 0\\
                                     1 & 1\\
                                 \end{pmatrix}
                          \right| = 1
        \end{aligned}
    \end{equation}

    We can now use the Jacobian combined with the fact that $Z_1$ and $Z_2$ are independent to obtain

    \begin{equation}
        f_{Z_1Z_2}(z_1, z_2) =
            \frac{1}{2\pi} \exp\left({ -\frac{z_1^2}{2} }\right) \exp\left({ -\frac{z_2^2}{2} }\right)
    \end{equation}

    In this case we see that if $x=z_1, y=z_2$, then $\rho=0, \mu_x=\mu_y=0, \sigma_x=1, \sigma_y=1$ and we obtain

    \begin{equation}
        \begin{aligned}
            f(x, y) &=&
                \frac{%
                \exp \left[
                    - \frac{1}{2\left( 1 - (0)^2 \right)}
                    \left(
                        {\left[ \frac{x - (0)}{(1)} \right]}^2 +
                        {\left[ \frac{y - (0)}{(1)} \right]}^2 -
                        2 (0) \frac{(x - (0))(y - (0))}{(1)(1)}
                    \right)
                \right]}{2\pi (1) (1) \sqrt{1 - (0)^2} }\\
            &=& \frac{%
                \exp \left[
                    - \frac{1}{2}
                    \left(
                        x^2 + y^2
                    \right)
                \right]}{2\pi}
        \end{aligned}
    \end{equation}

    @ \textit{Chapter 6, \#52} Let $X$ and $Y$ denote the coordinates of a point uniformly chosen in the circle of
    radius 1 centered at the origin. That is, their joint density is

    \begin{equation}
        f(x, y) = \frac{1}{\pi} \quad x^2 + y^2 \le 1
    \end{equation}

    Find the joint density function of the polar coordinates $R = (X^2 + Y^2 )^{1/2}$ and $\Theta = \tan^{âˆ’1} Y/X$.

    @@ This is fairly straightforward. We know the values of $r, \theta, x, y$.

    \begin{equation}
        \begin{aligned}
            r(x, y) = {(x^2 + y^2)}^{1/2} &\qquad& \theta(x, y) = \arctan \left( \frac{y}{x} \right)\\
            x = \sqrt{r^2 \cos^2 t} &\qquad& y = \frac{r \tan t}{\sqrt{1 + \tan^2 t} }
        \end{aligned}
    \end{equation}

    Solving the Jacobian yields

    \begin{equation}
        \begin{aligned}
            J(x, y) = \left| \begin{pmatrix}
                                 \frac{x}{\sqrt{x^2+y^2} } & \frac{y}{\sqrt{x^2+y^2} }\\
                                 \frac{-y}{x^2 \left(\frac{y^2}{x^2}+1\right)} & \frac{1}{x \left(\frac{y^2}{x^2}+1\right)}
                             \end{pmatrix}
                      \right| = \frac{1}{\sqrt{x^2+y^2} } = \frac{1}{r}
        \end{aligned}
    \end{equation}

    We can now plug in the parts and find the joint function.

    \begin{equation}
        \begin{aligned}
            f(r, \theta) = \frac{r}{\pi}
        \end{aligned}
    \end{equation}

    @ \textit{Chapter 6, \#54} If $U$ is uniform on $(0, 2\pi)$ and $Z$, independent of $U$, is exponential with rate 1,
    show directly (without using the results of Example 7b) that $X$ and $Y$ defined by

    \begin{equation}
        \begin{aligned}
            X &=& \sqrt{2Z}\cos U\\
            Y &=& \sqrt{2Z}\sin U
        \end{aligned}
    \end{equation}

    are independent standard normal random variables.

    @@ First let us define $U$ and $Z$.

    \begin{equation}
        \begin{aligned}
            U &=& \frac{1}{2 \pi} &\qquad 0 < u < 2 \pi\\
            Z &=& e^{-z} &\qquad z \ge 0
        \end{aligned}
    \end{equation}

    We can now establish $x(u, z) = \sqrt{2z}\cos u$ and $y(u, z) = \sqrt{2z}\sin u$, by which our Jacobian follows.

    \begin{equation}
        \begin{aligned}
            J(u, z) = \left| \begin{pmatrix}
                                     -\sqrt{2z} \sin u & \frac{\cos u}{\sqrt{2z} }\\
                                     \sqrt{2z} \cos u & \frac{\sin u}{\sqrt{2z} }\\
                                 \end{pmatrix}
                          \right| = -1
        \end{aligned}
    \end{equation}

    We can now (since the variables are independent) determine the joint density to be the following with
    $u=\arctan\left( y/x \right), z=\left( x^2 + y \right)/2$.

    \begin{equation}
        f(x, y) = \frac{e^{-\left( x^2 \right)/2} }{2 \pi} \qquad -\infty \le x \le \infty
    \end{equation}

    @ \textit{Chapter 6, \#55} $X$ and $Y$ have joint density function

    \begin{equation}
        f(x, y) = \frac{1}{x^2y^2} \quad x \ge 1, y \ge 1
    \end{equation}

    @@ Compute the joint density function of $U = XY, V = X/Y$.

    @@@ Establishing $u(x,y)=xy$ and $v(x,y)=x/y$, we find the Jacobian.

    \begin{equation}
        \begin{aligned}
            J(x, y) = \left| \begin{pmatrix}
                                     y & x\\
                                     1/y & -x/y^2
                                 \end{pmatrix}
                          \right| = -x/y - x/y = -2x/y = -2v
        \end{aligned}
    \end{equation}

    Solving for $x$ and $y$, we obtain $x=v\sqrt{u/v}$ and $y=\sqrt{u/v}$ which we can then use with our joint density
    function given above.

    \begin{equation}
        \begin{aligned}
            f(u, v) = \frac{1}{2u^2v}
        \end{aligned}
    \end{equation}

    @@ What are the marginal densities?

    @@@ To find these we integrate separetely and find

    \begin{equation}
        \begin{aligned}
            f_u(u) &=& \int^\infty_1
                \frac{1}{2u^2v}
                \, dv &=& \frac{1}{4u}\\
            f_v(v) &=& \int^\infty_1
                \frac{1}{2u^2v}
                \, du &=& \frac{1}{2v}
        \end{aligned}
    \end{equation}

    @ \textit{Chapter 6, \#56b} If $X$ and $Y$ are independent and identically distributed uniform random variables on
    (0, 1), compute the joint density of $U = X, V = X/Y$.

    @@ $X$ and $Y$ have the form

    \begin{equation}
        \begin{aligned}
            f_X(x) = f_Y(y) = 1 \qquad 0 \le \left\{ x,y \right\} \le 1
        \end{aligned}
    \end{equation}

    As before, we define $u(x,y)=x$ and $v(x,y)=x/y$ in order to find the Jacobian.

    \begin{equation}
        \begin{aligned}
            J(z,y) = \left| \begin{pmatrix}
                                1 & 0\\
                                1/y & -x/y^2\\
                            \end{pmatrix}
                          \right| = -x/y^2
        \end{aligned}
    \end{equation}

    Solving for $x$ and $y$ we obtain $x=u$ and $y=u/v$. We can now determine the joint distribution.

    \begin{equation}
        \begin{aligned}
            f(u, v) = \frac{-v^2}{u}
        \end{aligned}
    \end{equation}

    @ \textit{Chapter 6, \#57a} Repeat Problem 56.a when $X$ and $Y$ are independent exponential random variables, each
    with parameter $\lambda = 1$.

    @@ The referenced problem requests $U = X + Y, V = X/Y$. We know the form of $X, Y, x, y, u$, and $v$.

    \begin{equation}
        \begin{aligned}
            f_X(x) = e^{-x} &\qquad& f_Y(y) = e^{-y}\\
            u(x,y) = x + y &\qquad& v(x, y) = x/y\\
            x = \frac{uv}{v+1} &\qquad& y = \frac{u}{v+1}
        \end{aligned}
    \end{equation}

    We can now solve the for the Jacobian.

    \begin{equation}
        \begin{aligned}
            J(x, y) = \left| \begin{pmatrix}
                                 1 & 1\\
                                 1/y & -x/y^2\\
                             \end{pmatrix}
                      \right| = -x/y^2 - 1/y = -\frac{(u+1) (v+1)}{v}
        \end{aligned}
    \end{equation}

    Which gives us the joint distribution.

    \begin{equation}
        \begin{aligned}
            f(u, v) = \frac{v \exp\left({-\frac{(u+1) v}{v+1} }\right)}{(u+1) (v+1)}
        \end{aligned}
    \end{equation}

    @ \textit{Chapter 7, \#2} The game of Clue involves 6 suspects, 6 weapons, and 9 rooms. One of each is randomly
    chosen and the object of the game is to guess the chosen three.

    @@ How many solutions are possible? In one version of the game, the selection is made and then each of the players
    is randomly given three of the remaining cards. Let $S$, $W$, and $R$ be, respectively, the numbers of suspects,
    weapons, and rooms in the set of three cards given to a specified player. Also, let $X$ denote the number of
    solutions that are possible after that player observes his or her three cards.

    @@@ There are 6 suspects, 6 weapons, and 9 rooms, which is $6 \cdot 6 \cdot 9 = 324$.

    @@ Express $X$ in terms of $S$, $W$, and $R$.

    @@@ If the player has a certain amount of knowledge about the game, represented by how many cards they've seen, we
    can state the number of outcomes as

    \begin{equation}
        X = \left( 6 - S \right)\left( 6 - W \right)\left( 9 - R \right)
    \end{equation}

    @@ Find $E[X]$.

    @@@ $X$ has uniform distribution, which means that the probability of any particular solution occurring is equal to

    \begin{equation}
        \begin{aligned}
            S + W + R = 3 \qquad S,W,R \in \left\{ 0, 1, 2, 3 \right\}\\
            p(x) = \frac{1}{\binom{5}{2} } = \frac{1}{10}
        \end{aligned}
    \end{equation}

    We can now find the expected value

    \begin{equation}
        \begin{aligned}
            E[X] &=& \sum\limits_S
                       \sum\limits_W
                       \sum\limits_R
                       \left( \frac{1}{10} \right)(6-S)(6-W)(9-R)\\
                 &=& \sum_{s=0}^3
                       \sum_{w=0}^3
                       \sum_{r=0}^3
                       \left( \frac{1}{10} \right)(6-s)(6-w)(9-r)\\
                &=& 190.4
        \end{aligned}
    \end{equation}

    @ \textit{Chapter 7, \#9} A total of $n$ balls, numbered 1 through $n$, are put into $n$ urns, also numbered 1
    through $n$ in such a way that ball $i$ is equally likely to go into any of the urns $1, 2, \ldots, i$. Find

    @@ the expected number of urns that are empty

    @@@ Let $E$ be the number of empty urns, therefore $E$ is equal to the sum of $I$ where $I$ is a Bernoulli variable
    with values $\left\{ 0,1 \right\}$.

    \begin{equation}
        E = \sum\limits_I I
    \end{equation}

    The probability that any given urn $i$ is empty is

    \begin{equation}
        P(I_i) = \frac{i - 1}{n}
    \end{equation}

    Therefore

    \begin{equation}
        \begin{aligned}
            E[E] = \sum_{i=1}^n \frac{i - 1}{n} = \frac{n-1}{2}
        \end{aligned}
    \end{equation}

    @@ the probability that none of the urns is empty.

    @@@ In order for no urn to be left empty the $n$th ball \textit{must} go in the $n$th urn, which occurs with
    probability $1/n$. Similarly, the $(n-1)$th ball must go in the $(n-1)$th urn. Extending this we see that our
    probability is equal to

    \begin{equation}
        \prod_{k=1}^n \frac{1}{k}
    \end{equation}

    @ \textit{Chapter 7, \#10} Consider 3 trials, each having the same probability of success. Let $X$ denote the total
    number of successes in these trials. If $E[X] = 1.8$, what is the following. \newline

    In both cases, construct a probability scenario that results in $P\{X = 3\}$ having the stated value.

    @@ the largest possible value of $P\{X = 3\}$?

    @@@ We can solve this equation

    \begin{equation}
            0 \cdot P\left\{ X=0 \right\} +
            1 \cdot P\left\{ X=1 \right\} +
            2 \cdot P\left\{ X=2 \right\} +
            3 \cdot P\left\{ X=3 \right\} = 1.8
    \end{equation}

    If we wish to find the largest possible value of $P\left\{ X=3 \right\}$ we can maximize it in our equation by
    letting all other probabilities be zero, yielding

    \begin{equation}
        \begin{aligned}
            3 \cdot P\left\{ X=3 \right\} &=& 1.8\\
            P\left\{ X=3 \right\}&=& 0.6
        \end{aligned}
    \end{equation}

    @@ the smallest possible value of $P\{X = 3\}$?

    @@@ To find the smallest value of $P\left\{ X=3 \right\}$ we can let it be zero, and then find the other
    probabilities which sum to 1. This equation combined with our expected value yields a system of equations.

    \begin{equation}
        \begin{aligned}
            \begin{cases}
                P\left\{ X=0 \right\} +
                P\left\{ X=1 \right\} +
                P\left\{ X=2 \right\} = 1\\
                0 \cdot P\left\{ X=0 \right\} +
                1 \cdot P\left\{ X=1 \right\} +
                2 \cdot P\left\{ X=2 \right\} = 1.8
            \end{cases}\\
        \end{aligned}
    \end{equation}

    Which we can now solve.

    \begin{equation}
        \begin{aligned}
            P\left\{ X=1 \right\} &=& 1.8 - 2 \cdot P\left\{ X=2 \right\}\\
            P\left\{ X=0 \right\} + 1.8 - 2 \cdot P\left\{ X=2 \right\} + P\left\{ X=2 \right\} &=& 1\\
            P\left\{ X=0 \right\} - P\left\{ X=2 \right\} &=& -0.8\\
        \end{aligned}
    \end{equation}

    One solution to the problem yields probabilities (which minimize $P\left\{ X=3 \right\}$)

    \begin{equation}
        \begin{aligned}
            P\left\{ X=0 \right\} = 0\\
            P\left\{ X=1 \right\} = 0.2\\
            P\left\{ X=2 \right\} = 0.8\\
            P\left\{ X=3 \right\} = 0\\
        \end{aligned}
    \end{equation}

    @ You are traveling from San Francisco to Casper, Wyoming, with a change of planes in Denver.  Your arrival time in
    Denver is uniformly distributed between 12:00 noon and 1:00 PM. Your connecting flight leaves Denver between 12:30
    and 1:30 PM and the departure time is also uniformly distributed and independent of the arrival time.

    @@ Assuming you require 20 minutes to change planes, what is the probability that you will make the connection?

    @@@ If we let $A$ be the independent variable representing arrival time (in minutes since 12), and $D$ the departure
    time (similarly in minutes since 12) we can restate the problem as $1 - P\left\{ D - A \le 20 \right\}$. We can
    solve for this distribution.

    \begin{equation}
        \begin{aligned}
            F_{D-A}(n) &=& P\left\{ D - A \le n \right\}\\
                       &=& \iint\limits_{d - a \le n} \frac{1}{3600} \, dd \, da\\
                       &=& \iint\limits_{d \le n + a} \frac{1}{3600} \, dd \, da\\
                       &=& \int_0^{60} \int_0^{n+a} \frac{1}{3600} \, dd \, da\\
                       &=& \frac{1}{2} + \frac{n}{60}\\
       1 - F_{D-A}(20) &=& 0.1667
        \end{aligned}
    \end{equation}

    @@ Given that your arrival is before 12:30 PM, what is the probability that you will make the connection? (Remember,
    you still need 20 minutes to change planes.)

    @@@ Now we can simply adjust our limits of integration now in order to find the answer.

    \begin{equation}
        \begin{aligned}
            F_{D-A}(n) &=& P\left\{ D - A \le n \right\}\\
                       &=& \iint\limits_{d - a \le n} \frac{1}{3600} \, dd \, da\\
                       &=& \iint\limits_{d \le n + a} \frac{1}{3600} \, dd \, da\\
                       &=& \int_0^{30} \int_0^{n+a} \frac{1}{3600} \, dd \, da\\
                       &=& \frac{1}{8} + \frac{n}{120}\\
       1 - F_{D-A}(20) &=& 0.7083
        \end{aligned}
    \end{equation}

    @@ What is the average amount of time you must wait in Denver? (Waiting does not include the 20 minutes needed to
    make the connection and, if you miss the flight, assume your waiting time is zero.)

    @@@ This is the same question as asking the expected value of $D - A - 20$, since we are interested in the wait
    time, but minus the 20 minute boarding time. If the value $D - A - 20$ is less than zero, we can assume we missed
    the flight and the wait time is zero. We need the pdf of this function of two random variables.

    \begin{equation}
        \begin{aligned}
            F_{D - A - 20}(n) &=& P\left\{ D - A - 20 \le n \right\}\\
                              &=& \iint\limits_{d - a - 20 \le n} \frac{1}{3600} \, dd \, da\\
                              &=& \int_0^{60} \int_0^{n + a + 20} \frac{1}{3600} \, dd\,da\\
            F_{D - A - 20}(n) &=& \frac{5}{6} + \frac{n}{60}\\
            f(n) &=& \frac{1}{60}
        \end{aligned}
    \end{equation}

    This tells us the the probability of the waiting time $N$ being equal to $n$ is a uniform distribution. This makes
    sense given that our $D$ and $A$ are also uniform distributions. We now need to calculate the expected value, which
    is given by

    \begin{equation}
        \begin{aligned}
            \int_0^{60} \frac{x}{60} \, dx
        \end{aligned}
    \end{equation}

    Meaning our expected value is equal to $30$ minutes of waiting.

    @@ Repeat parts (a), (b), and (c) under the assumption that your arrival time has some distribution given by a
    density function $f_a$ (which is nonzero only between noon and 1) and the departure time has density function $f_d$
    (nonzero only between 12:30 and 1:30). In other words, make sure you can set up the appropriate integrals (with the
    correct limits of integration) to calculate the probabilities requested in parts (a), (b), and (c).

    @@@ If $A$ and $D$ now have non-uniform distributions, this makes things much trickier. For part (a) we still need
    the given quantity $1 - P\left\{ D - A \le 20 \right\}$, but the equation now looks like the following.

    \begin{equation}
        \begin{aligned}
            F_{D - A}(n) &=& P\left\{ D - A \le n \right\}\\
                         &=& \iint\limits_{d - a \le n} f_a(a) \cdot f_d(d) \, dd \, da\\
                         &=& \iint\limits_{d \le n + a} f_a(a) \cdot f_d(d) \, dd \, da\\
                         &=& \int_0^{60} \int_0^{n+a} f_a(a) \cdot f_d(d) \, dd \, da\\
            && \boxed{1 - \int_0^{60} \int_0^{n+a} f_a(a) \cdot f_d(d) \, dd \, da}\\
        \end{aligned}
    \end{equation}

    Now we need the same equation, save if $A$ is less than 30. Again, this is fairly straightforward.

    \begin{equation}
        \begin{aligned}
            F_{D-A}(n) &=& P\left\{ D - A \le n \right\}\\
                       &=& \iint\limits_{d - a \le n} f_a(a) \cdot f_d(d) \, dd \, da\\
                       &=& \iint\limits_{d \le n + a} f_a(a) \cdot f_d(d) \, dd \, da\\
                       &=& \int_0^{30} \int_0^{n+a} f_a(a) \cdot f_d(d) \, dd \, da\\
                       && \boxed{1 - \int_0^{30} \int_0^{n+a} f_a(a) \cdot f_d(d) \, dd \, da}\\
        \end{aligned}
    \end{equation}

    Now the expected value is a little trickier.

    \begin{equation}
        \begin{aligned}
            F_{D - A - 20}(n) &=& P\left\{ D - A - 20 \le n \right\}\\
                              &=& \iint\limits_{d - a - 20 \le n} f_a(a) \cdot f_d(d) \, dd \, da\\
                              &=& \int_0^{60} \int_0^{n + a + 20} f_a(a) \cdot f_d(d) \, dd\,da\\
                         f(n) &=& \frac{d}{dn} \left( \int_0^{60} \int_0^{n + a + 20} f_a(a) \cdot f_d(d) \, dd\,da \right)\\
        \end{aligned}
    \end{equation}

    This gives us our pdf, but we need the expected value, which is

    \begin{equation}
        \begin{aligned}
            \int_0^{60} n \cdot \left(
                 \frac{d}{dn} \left(
                     \int_0^{60} \int_0^{n + a + 20} f_a(a) \cdot f_d(d) \, dd\,da
                 \right)
             \right) \, dn
        \end{aligned}
    \end{equation}

\end{easylist}

\end{document}
