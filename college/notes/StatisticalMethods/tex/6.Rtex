\documentclass[10pt]{article}

\input{./tex/header.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beginning of document items - headers, title, toc, etc...
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}                                                       %  Establishes that the headers will be defined
\fancyhead[LE,LO]{Homework 6}                                  %  Adds header to left
\fancyhead[RE,RO]{Zoe Farmer}                                       %  Adds header to right
\cfoot{\mlptikz[size=0.25in, text=on, textposx=0, textposy=0, textvalue=\thepage, textscale=0.75in]{applejack} }
\lfoot{APPM 4570}
\rfoot{Hagar}
\title{Homework 6}
\author{Zoe Farmer}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beginning of document items - headers, title, toc, etc...
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\maketitle

<<echo=F, message=F>>=
    library('ggplot2')
    library('Devore7')
    library('gridExtra')
@

In addition to the questions asked, please state the estimated model, interpret all parameters in the model, and test
the linear regression assumptions (using diagnostic plots\footnote{Some possibilities are e* vs $\hat{y}$, $y$ vs
$\hat{y}$, hist(e*), or x vs y})

\begin{easylist}[enumerate]
    @ \textbf{Chapter 12, \#1}  %% DONE

    @@ Plot a histogram of the data.

    <<12.1a, tidy=F, fig.width=3, fig.height=3, fig.align='center', fig.pos='H', fig.show='hold', out.width='0.45\\textwidth'>>=
             ggplot(ex12.01, aes(Temp)) +
                 geom_histogram(aes(y=..density..), binwidth=2,
                                color='black', fill='white')
             ggplot(ex12.01, aes(Ratio)) +
                 geom_histogram(aes(y=..density..), binwidth=0.5,
                                color='black', fill='white')
    @

    @@ Is the value of efficiency ratio completely and uniquely determined by tank temperature?\newline

    No. If you examine the data as well as the scatterplot below, we see that even when the temperature remains
    constant, our ratio fluctuates. On the other hand, if we do not care very much about error, we see that the ratio is
    always within 1 ratio unit away from our regression line.

    @@ Does it appear that efficiency ratio could be very well predicted by the value of temperature?

    <<12.1c, tidy=F, fig.width=3, fig.height=3, fig.align='center', fig.show='hold'>>=
             ggplot(ex12.01, aes(x=Temp, y=Ratio)) +
                geom_point() +
                geom_smooth(method=lm)
    @

    Using our scatterplot with a linear regression line added, we see that while there is some linear relation between
    the temperature and the ratio, the values vary far too much for our temperature to be a good predictor.

    @@ State the estimated model, interpret its parameters, and test the linear regression assumptions using diagnostic
    plots.\newline

    We can determine the coefficients $\hat{\beta}_0$ and $\hat{\beta}_1$ from the regression line used above.

    <<tidy=F>>=
             model <- lm(Ratio~Temp, data=ex12.01)
             cof <- coefficients(model)
             reg_line <- function(x) { cof[1] + cof[2] * x }
    @

    Yielding the equation $\boxed{Ratio(Temp=x) = \Sexpr{cof[1]} + \Sexpr{cof[2]} \cdot x}$. Where $\hat{\beta}_0$ is
    the $y$-intercept and $\hat{\beta}_1$ is the slope of our linear regression.\newline

    We can also test our linear regression using diagnostic plots. We've already seen {\ttfamily Temp} vs. {\ttfamily
    Ratio}, so we'll examine the others.

    <<12.1.diagnostic, tidy=F, fig.width=3, fig.height=2, fig.align='center', fig.show='hold', out.width='0.45\\textwidth', fig.pos='H'>>=
             x <- ex12.01$Temp
             y <- ex12.01$Ratio
             y_hat <- reg_line(x)
             raw_residuals <- (y - y_hat)
             residuals <- (raw_residuals - mean(raw_residuals)) /
                             sd(raw_residuals)
             diagnostic <- data.frame(x=x, y=y, y_hat=y_hat,
                                      residuals=residuals)
             ggplot(diagnostic, aes(residuals)) +
                 geom_histogram(aes(y=..density..), binwidth=1,
                                color='black', fill='white')
             ggplot(diagnostic, aes(y=residuals, x=x)) + geom_point()
             ggplot(diagnostic, aes(y=residuals, x=y_hat)) + geom_point()
             ggplot(diagnostic, aes(x=y, y=y_hat)) + geom_point()
    @

    Since our standardized residuals appear to be normally distributed and unbiased, our model is probably a good fit.

    @ \textbf{Chapter 12, \#14} - With the same data as above

    @@ Determine the equation of the estimated regression line.\newline

    To find the equation, we need $\hat{\beta}_1$ and $\hat{\beta}_0$.

    \begin{equation}
        \begin{aligned}
            \hat{\beta}_1 &=& \frac{S_{xy} }{S_{xx} } =
                \frac{\sum x_i y_i - \left( \sum x_i \right)\left( \sum y_i \right)/n}
                {\sum x^2_i - {\left( \sum x_i \right)}^2/n}\\
            \hat{\beta}_0 &=& \overline{y} - \hat{\beta}_1 \overline{x}
        \end{aligned}
    \end{equation}

    We can calculate these points.

    <<12.14a, tidy=F>>=
             temp <- ex12.01$Temp
             ratio <- ex12.01$Ratio
             n <- length(temp)
             b_1 <- (sum(temp * ratio) - (sum(temp) * sum(ratio))/n) /
                     (sum(temp^2) - (sum(temp)^2)/n)
             b_0 <- mean(ratio) - b_1 * mean(temp)
             reg_line <- function(x) { b_0 + b_1 * x }
    @

    Which yields $\boxed{\left\{ \hat{\beta}_0 \to \Sexpr{b_0}, \hat{\beta}_1 \to \Sexpr{b_1} \right\} }$, and the
    equation $y = \Sexpr{b_0} + \Sexpr{b_1} x$.

    @@ Calculate a point estimate for true average efficiency ratio when tank temperature is 182.\newline

    We defined our function above, so this is merely plugging in 182 into our function and obtain
    $\Sexpr{reg_line(182)}$, which is on par with our regression line given by ggplot above.

    @@ Calculate the values of the residuals from the least squares line for the four observations for which temperature
    is 182. Why do they not all have the same sign?\newline

    This can be accomplished with R.

    <<12.14c, tidy=F>>=
             y_hat <- reg_line(182)
             residuals <- ratio[temp == 182] - y_hat
    @

    And we obtain the residuals to be $\left\{\Sexpr{residuals}\right\}$. These aren't all the same sign because the
    residual is an indicator of where the sample data lies compared to the true regression line.

    @@ What proportion of the observed variation in efficiency ratio can be attributed to the simple linear regression
    relationship between the two variables?\newline

    This can be expressed by the coefficient of determination, $r^2$.

    \begin{equation}
        \begin{aligned}
            r^2 &=& 1 - \frac{SSE}{SST}\\
            SSE &=& \sum y_i^2 - \hat{\beta}_0 \sum y_i - \hat{\beta}_1 \sum x_i y_i\\
            SST &=& \sum y_i^2 - \frac{ {\left( \sum y_i \right)}^2}{n}\\
        \end{aligned}
    \end{equation}

    These values are calculable.

    <<tidy=F>>=
             sse <- sum(ratio^2) - b_0 * sum(ratio) - b_1 * sum(temp * ratio)
             sst <- sum(ratio^2) - (sum(ratio)^2)/n
             r2 <- 1 -(sse / sst)
    @

    Yielding $r^2 = \Sexpr{r2}$. This value means that about half the observed $y$ variation is explained by the
    regression model.

    @@ State the estimated model, interpret its parameters, and test the linear regression assumptions using diagnostic
    plots.\newline

    We've already determined the model to be the equation $y = \Sexpr{b_0} + \Sexpr{b_1} x$, and the interpretation for
    the parameters is the same as in problem 1. Since the data for this problem and problem 1 are the same, the same
    diagnostic plots apply.

    @ \textbf{Chapter 12, \#24}

    <<tidy=F>>=
          x <- ex12.24$x
          y <- ex12.24$y
    @

    @@ Construct a scatterplot. Does simple linear regression appear to be reasonable?\newline

    <<12.24a, tidy=F, fig.width=3, fig.height=3, fig.align='center', fig.pos='H'>>=
             ggplot(ex12.24, aes(x=x, y=y)) +
                 geom_point() +
                 geom_smooth(method=lm)
    @

    In this case it appears that simple linear regression is probably a reasonable estimate of our data, given our
    outlying point is not an outlier. This is improbable, so linear regression may not be the best fit of our data. All
    this being said however, we do not have a very large sample size, so no model will be very accurate.

    @@ Calculate the equation of the estimated regression line.\newline

    We can adopt the same process as we used before.

    <<tidy=F>>=
             n <- length(x)
             b_1 <- (sum(x * y) - (sum(x) * sum(y))/n) /
                     (sum(x^2) - (sum(x)^2)/n)
             b_0 <- mean(y) - b_1 * mean(x)
    @

    Which yields $\boxed{\left\{ \hat{\beta}_0 \to \Sexpr{b_0}, \hat{\beta}_1 \to \Sexpr{b_1} \right\} }$, and the
    equation $y = \Sexpr{b_0} + \Sexpr{b_1} x$.

    @@ What percentage of observed variation in steel weight loss can be attributed to the model relationship in
    combination with variation in deposition rate?\newline

    This is again our coefficient of determination, which is also calculated using R.

    <<tidy=F>>=
             sse <- sum(y^2) - b_0 * sum(y) - b_1 * sum(x * y)
             sst <- sum(y^2) - (sum(y)^2)/n
             r2 <- 1 -(sse / sst)
    @

    Yielding $r^2 = \Sexpr{r2}$, indicating that a large degree of steel weight loss can be attributed to the model
    relationship. In other words, our linear model fits the data very well as approximately $98\%$ of the data's
    variance can be explained by the given variables.

    @@ Because the largest x value in the sample greatly exceeds the others, this observation may have been very
    influential in determining the equation of the estimated line. Delete this observation and recalculate the equation.
    Does the new equation appear to differ substantially from the original one (you might consider predicted values)?

    <<12.24d, tidy=F, fig.width=3, fig.height=3, fig.align='center', fig.pos='H'>>=
             new_data = data.frame(x=x[x != 112], y=y[x != 112])
             ggplot(new_data, aes(x=x, y=y)) +
                 geom_point() +
                 geom_smooth(method=lm)
             y <- y[x != 112]
             x <- x[x != 112]
             n <- length(x)
             b_1 <- (sum(x * y) - (sum(x) * sum(y))/n) /
                     (sum(x^2) - (sum(x)^2)/n)
             b_0 <- mean(y) - b_1 * mean(x)
    @

    Which yields $\boxed{\left\{ \hat{\beta}_0 \to \Sexpr{b_0}, \hat{\beta}_1 \to \Sexpr{b_1} \right\} }$, and the
    equation $y = \Sexpr{b_0} + \Sexpr{b_1} x$, which differs quite substantially from our original regression line.

    @@ State the estimated model, interpret its parameters, and test the linear regression assumptions using diagnostic
    plots.\newline

    We already determined the model above with outliers removed, so we can just straight to the diagnostic plots.

    <<12.24.diagnostic, tidy=F, fig.width=3, fig.height=2, fig.align='center', fig.show='hold', out.width='0.45\\textwidth', fig.pos='H'>>=
             reg_line <- function(x) { b_0 + b_1 * x }
             y_hat <- reg_line(x)
             raw_residuals <- y - y_hat
             residuals <- (raw_residuals - mean(raw_residuals)) /
                             sd(raw_residuals)
             diagnostic <- data.frame(x=x, y=y, y_hat=y_hat,
                                      residuals=residuals)
             ggplot(diagnostic, aes(residuals)) +
                 geom_histogram(aes(y=..density..), binwidth=1,
                                color='black', fill='white')
             ggplot(diagnostic, aes(y=residuals, x=x)) + geom_point()
             ggplot(diagnostic, aes(y=residuals, x=y_hat)) + geom_point()
             ggplot(diagnostic, aes(x=y, y=y_hat)) + geom_point()
    @

    Again, our residuals appear to be unbiased, and potentially normally distributed, so the model is probably a good
    fit, however again, since our sample size is so small it is impossible to tell for sure.

    @ \textbf{Chapter 12, \#46}

    <<tidy=F>>=
           data <- read.csv('./wines.csv')
           tan <- data$tannin
           ast <- data$astringency
           n <- length(tan)
    @

    @@ Fit the simple linear regression model to this data. Then determine the proportion of observed variation in
    astringency that can be attributed to the model relationship between astringency and tannin concentration.\newline

    <<12.46a, tidy=F, fig.width=3, fig.height=3, fig.align='center', fig.pos='H'>>=
             line <- lm(ast ~ tan)
             ggplot(data, aes(x=tannin, y=astringency)) +
                 geom_point() +
                 geom_smooth(method=lm)
                 co <- coefficients(line)
    @

    The regression line for this data is $y = \Sexpr{co[1]} + \Sexpr{co[2]}x$.\newline

    The coefficient of determination in this case is calculated in the same method as above.

    <<tidy=F>>=
             sse <- sum(ast^2) - co[1] * sum(ast) - co[2] * sum(tan * ast)
             sst <- sum(ast^2) - (sum(ast)^2)/n
             r2 <- 1 - (sse / sst)
    @

    Which yields $r^2 = \Sexpr{r2}$, indicating a large degree of observed variation. This means that our model is
    probably a good fit, as a large amount of variance can be explained by it.

    @@ State the estimated model, interpret its parameters, and test the linear regression assumptions using diagnostic
    plots.\newline

    We've already established our model above.

    <<12.46.diagnostic, tidy=F, fig.width=3, fig.height=2, fig.align='center', fig.show='hold', out.width='0.45\\textwidth', fig.pos='H'>>=
             reg_line <- function(x) { co[1] + co[2] * x }
             x <- tan
             y <- ast
             y_hat <- reg_line(x)
             raw_residuals <- (y - y_hat)
             residuals <- (raw_residuals - mean(raw_residuals)) /
                             sd(raw_residuals)
             diagnostic <- data.frame(x=x, y=y, y_hat=y_hat,
                                      residuals=residuals)
             ggplot(diagnostic, aes(residuals)) +
                 geom_histogram(aes(y=..density..), binwidth=1,
                                color='black', fill='white')
             ggplot(diagnostic, aes(y=residuals, x=x)) + geom_point()
             ggplot(diagnostic, aes(y=residuals, x=y_hat)) + geom_point()
             ggplot(diagnostic, aes(x=y, y=y_hat)) + geom_point()
    @

    This is probably the best fit data so far. Our residuals are normally distributed and unbiased, while our $y$ and
    $\hat{y}$ are strongly linked. All of this indicates a high probability that our regression is accurate.

    @@ Calculate and interpret a confidence interval for the slope of the true regression line.\newline

    As other with confidence intervals we use a probability, save in this case we use the $t$ distribution with
    $\alpha=0.05$.

    \begin{equation}
        \begin{aligned}
            1 - \alpha &=& P\left\{ -t_{\alpha / 2, n - 2} <
                \frac{\hat{\beta}_1 - \beta_1}{S_{\hat{\beta}_1} } <
                t_{\alpha/2, n - 2} \right\}\\
            &=& \hat{\beta}_1 \pm t_{\alpha / 2, n - 2} \cdot S_{\hat{\beta}_1}\\
            S_{\hat{\beta}_1} &=& \frac{\sqrt{\sum {\left( y - \hat{y} \right)}^2 / (n - 2)} }
                                        {\sqrt{\sum {\left( x - \overline{x} \right)}^2} }
        \end{aligned}
    \end{equation}

    Which we can express with R.

    <<tidy=F>>=
             alpha = 0.05
             b_1 <- co[2]
             t <- qt(1 - alpha / 2, n - 2)
             s <- sqrt(sum((y - y_hat)^2) / (n - 2)) /
                    sqrt(sum((x - mean(x))^2))
             lower <- b_1 - t * s
             upper <- b_1 + t * s
    @

    Yielding a confidence interval of $\boxed{\left[ \Sexpr{lower}, \Sexpr{upper} \right]}$.

    @@ Estimate true average astringency when tannin concentration is 0.6, and do so in a way that conveys information
    about reliability and precision.\newline

    Using our model from before we can estimate the true average astringency to be $\Sexpr{reg_line(0.6)}$, which is our
    $\hat{y}$. We can determine the variance to be

    \begin{equation}
        \begin{aligned}
            \sigma^2 \left[
                \frac{1}{n} + \frac{ {\left( x^* - \overline{x} \right)}^2}
                                {\sum x_i^2 - \left( \sum x_i \right)^2 / n}
            \right]
        \end{aligned}
    \end{equation}

    With a corresponding confidence interval

    \begin{equation}
        \begin{aligned}
            \hat{y} \pm t_{\alpha / 2, n - 2} \cdot S_{\hat{y} }
        \end{aligned}
    \end{equation}

    Using R we can determine these values.

    <<tidy=F>>=
             alpha <- 0.05
             y_hat <- reg_line(0.6)
             s <- sd(y) * sqrt((1/n) +
                          (0.6 - mean(x))^2/(sum(x^2) - (sum(x)^2/n)))
             t <- qt(1 - alpha / 2, n - 2)
    @

    Therefore astringency is equal to $\Sexpr{y_hat} \pm \Sexpr{t * s}$ with 95\% confidence.

    @@ Predict astringency for a single wine sample whose tannin concentration is 0.6, and do so in a way that conveys
    information about reliability and precision.\newline

    Using a similar method we know that our prediction interval is equal to

    \begin{equation}
        \begin{aligned}
            \hat{y} \pm t_{\alpha / 2, n - 2} \cdot \sqrt{s^2 + s_{\hat{y} }^2}
        \end{aligned}
    \end{equation}

    Which we can calculate with R.

    <<tidy=F>>=
             alpha <- 0.05
             y_hat <- reg_line(0.6)
             s <- sd(y)
             sy <- sd(reg_line(x))
             t <- qt(1 - alpha / 2, n - 2)
    @

    Yielding a prediction interval equal to $\Sexpr{y_hat} \pm \Sexpr{t * sqrt(s^2 + sy^2)}$ with 95\% confidence.

    @@ Does it appear that true average astringency for a tannin concentration of 0.7 is something other than 0? State
    and test the appropriate hypotheses.\newline

    We wish to test if the true astringency at 0.7 is equal to 0, meaning our null hypotheses is that the true
    astringency at 0.7 is equal to 0.\newline

    In this situation our test statistic is equal to $\hat{y}(0.7)$, which is equal to $\Sexpr{reg_line(0.7)}$. We can
    standardize this value (by subtracting the mean of $\hat{y}$ and dividing by the standard deviation), obtaining
    $\Sexpr{(reg_line(0.7) - mean(reg_line(x))) / sd(reg_line(x))}$. Since this is a two tailed test, we can compare
    this value to our $t$ distribution and see that $\Sexpr{qt(0.05/2, n-2)} \le
    \Sexpr{(reg_line(0.7) - mean(reg_line(x))) / sd(reg_line(x))}$, indicating that our null hypotheses is probably
    false, and that the true astringency at 0.07 is probably something other than 0.\newline

    If we examine our data, we see that this is apparent in the data as well.

    @ \textbf{Chapter 12, \#52}

    @@ Does the simple linear regression model specify a useful relationship between chlorine flow and etch
    rate?\newline

    Let's plot the data with a linear regression model.

    <<12.52a, tidy=F, fig.width=3, fig.height=3, fig.align='center', fig.pos='H'>>=
             ggplot(ex12.52, aes(x=x, y=y)) +
                geom_point() +
                geom_smooth(method=lm)
    @

    Based on the plot it certainly looks like it does, but let's examine our diagnostic plots as well.

    <<12.52.diagnostic, tidy=F, fig.width=3, fig.height=2, fig.align='center', fig.show='hold', out.width='0.45\\textwidth', fig.pos='H'>>=
             x <- ex12.52$x
             y <- ex12.52$y
             n <- length(x)

             line <- lm(y~x)
             co <- coefficients(line)
             reg_line <- function(x) { co[1] + co[2] * x }

             y_hat <- reg_line(x)
             raw_residuals <- (y - y_hat)
             residuals <- (raw_residuals - mean(raw_residuals)) /
                             sd(raw_residuals)
             diagnostic <- data.frame(x=x, y=y, y_hat=y_hat,
                                      residuals=residuals)
             ggplot(diagnostic, aes(residuals)) +
                 geom_histogram(aes(y=..density..), binwidth=0.5,
                                color='black', fill='white')
             ggplot(diagnostic, aes(y=residuals, x=x)) + geom_point()
             ggplot(diagnostic, aes(y=residuals, x=y_hat)) + geom_point()
             ggplot(diagnostic, aes(x=y, y=y_hat)) + geom_point()
    @

    Given our diagnostic plots with regression model $y=\Sexpr{co[1]} + \Sexpr{co[2]} \cdot x$, we can see that a linear
    regression model is a useful model in this case.

    @@ Estimate the true average change in etch rate associated with a 1-SCCM increase in flow rate using a 95\%
    confidence interval, and interpret the interval.\newline

    This is asking for the estimation of $\hat{\beta}_1$, the slope parameter with 95\% confidence. We've already
    determined our parameter based on the data, therefore our interval is defined as

    \begin{equation}
        \begin{aligned}
            \hat{\beta}_1 \pm t_{\alpha / 2, n - 2} \cdot s_{\hat{\beta}_1}\\
            s_{\hat{\beta}_1} = \frac{s}{\sqrt{S_{xx} } }
        \end{aligned}
    \end{equation}

    Which we can calculate with R.

    <<tidy=F>>=
             alpha <- 0.05
             b1 <- co[2]
             s <- sd(y)
             sb <- s / sqrt(var(x))
             t <- qt(1 - alpha / 2, n - 2)
             err <- t * sb
    @

    Yielding an estimate for $\hat{\beta}_1$ equal to $\Sexpr{b1} \pm \Sexpr{err}$ with 95\% confidence. This indicates
    that 95\% of the time the calculated interval for a subset of data will contain the true slope parameter.

    @@ Calculate a 95\% CI for $\mu_{Y \cdot 3.0}$ , the true average etch rate when flow = 3.0. Has this average been
    precisely estimated?\newline

    We know our interval is defined as

    \begin{equation}
        \begin{aligned}
            \hat{y} \pm t_{\alpha / 2, n - 2} \cdot s_{\hat{y} }\\
            s_{\hat{y} } = s \sqrt{\frac{1}{n} + \frac{ {\left( x^* - \overline{x} \right)}^2}{S_{xx} } }
        \end{aligned}
    \end{equation}

    Which we can calculate with R.

    <<tidy=F>>=
             reg_line <- function(x) { co[1] + co[2] * x }
             xs <- 3
             y_hat <- reg_line(xs)
             s <- sd(y)
             sy <- s * sqrt((1/n) + ((xs - mean(x))^2)/var(x))
             t <- qt(1 - 0.05 / 2, n - 2)
    @

    Which yields an estimate for $\mu_{Y\cdot x^*}$ equal to $\Sexpr{y_hat} \pm \Sexpr{sy * t}$.

    @@ Calculate a 95\% PI for a single future observation on etch rate to be made when flow = 3.0 . Is the prediction
    likely to be accurate?\newline

    Simarly, we can create a prediction interval for the same $x^*$ using the formula

    \begin{equation}
        \begin{aligned}
            \hat{y} \pm t_{\alpha / 2, n - 2} \cdot \sqrt{s^2 + s^2_{\hat{y} } }
        \end{aligned}
    \end{equation}

    Calculating with R we obtain

    <<tidy=F>>=
             xs <- 3
             y_hat <- reg_line(xs)
             t <- qt(1 - 0.05 / 2, n - 2)
             s <- sd(y)
             sy <- sd(reg_line(x))
             err <- sqrt(s^2 + sy^2)
    @

    Yielding the prediction interval for $\mu_{Y \cdot 3}$ equal to $\Sexpr{y_hat} \pm \Sexpr{err}$.

    @@ Would the 95\% CI and PI when flow = 2.5 be wider or narrower than the corresponding intervals of parts (c) and
    (d)? Answer without actually computing the intervals.\newline

    Since there are points there and we can rely on data as well as the regression model, the intervals should be
    smaller.

    @@ Would you recommend calculating a 95\% PI for a flow of 6.0? Explain.\newline

    Yes. In this situation a CI would not work, as it does not take into account the imprecision of the model. A
    Prediction Interval on the other hand remains wide enough to account for any error in the model.

    @ \textbf{Chapter 13, \#13 --} Recall that $\hat{\beta}_0 + \hat{\beta}_1 x$ has a normal distribution with expected
    value $\beta_0 + \beta_1 x$ and variance

    \begin{equation}
        \sigma^2 \left\{ \frac{1}{n} + \frac{ {\left( x - \overline{x} \right)}^2}
                 {\sum { \left( x_i - \overline{x} \right)}^2} \right\}
    \end{equation}

    so that

    \begin{equation}
        Z = \frac{\hat{\beta}_0 + \hat{\beta}_1 x - \left( \beta_0 + \beta_1 x \right)}
        {\sigma { \left( \frac{1}{n} + \frac{ {\left( x - \overline{x} \right)}^2}
                        {\sum { \left( x_i - \overline{x} \right)}^2} \right) }^2}
    \end{equation}

    has a standard normal distribution. If $S = \sqrt{SSE/(n-2)}$ is substituted for $\sigma$, the resulting
    variable has a $t$ distribution with $n - 2$ df. By analogy, what is the distribution of any particular standardized
    residual? If $n = 25$, what is the probability that a particular standardized residual falls outside the interval
    $(-2.50, 2.50)$?\newline

    Any standardized residual also has $t$ distribution with $n - 2$ degrees of freedom. If we let $n=25$, then the
    probability that a particular standardized residual falls outside the given interval is equal to
    $P\left\{ R < -2.5 \right\} + P\left\{ R > 2.5 \right\}$, which can be calculated with R, yielding
    $\Sexpr{pt(-2.5,23)+(1-pt(2.5,23))}$.
\end{easylist}

\newpage

\end{document}
